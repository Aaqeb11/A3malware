{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tkinter import messagebox\n", "from tkinter import *\n", "from tkinter.filedialog import askopenfilename\n", "from tkinter import simpledialog\n", "import tkinter\n", "import numpy as np\n", "from tkinter import filedialog\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score\n", "import matplotlib.pyplot as plt\n", "from sklearn.naive_bayes import BernoulliNB\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.metrics import precision_score\n", "from sklearn.metrics import recall_score\n", "from sklearn.metrics import f1_score\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn import svm\n", "from keras.models import Sequential\n", "from keras.layers import Convolution2D\n", "from keras.layers import MaxPooling2D\n", "from keras.layers import Flatten\n", "from keras.layers import Dense, Activation, BatchNormalization, Dropout\n", "from sklearn.preprocessing import OneHotEncoder\n", "from keras.models import model_from_json\n", "# from keras.layers import LSTM\n", "from keras.layers.embeddings import Embedding\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.preprocessing import Normalizer\n", "import keras.layers\n", "from keras.models import model_from_json"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["main = tkinter.Tk()\n", "main.title(\"Robust Intelligent Malware Detection Using Deep Learning\")\n", "main.geometry(\"1300x1200\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["malware_name = ['Dialer Adialer.C', 'Backdoor Agent.FYI', 'Worm Allaple.A', 'Worm Allaple.L', 'Trojan Alueron.gen',\n", "                'Worm:AutoIT Autorun.K',\n", "                'Trojan C2Lop.P', 'Trojan C2Lop.gen', 'Dialer Dialplatform.B', 'Trojan Downloader Dontovo.A',\n", "                'Rogue Fakerean', 'Dialer Instantaccess',\n", "                'PWS Lolyda.AA 1', 'PWS Lolyda.AA 2', 'PWS Lolyda.AA 3', 'PWS Lolyda.AT', 'Trojan Malex.gen',\n", "                'Trojan Downloader Obfuscator.AD',\n", "                'Backdoor Rbot!gen', 'Trojan Skintrim.N', 'Trojan Downloader Swizzor.gen!E',\n", "                'Trojan Downloader Swizzor.gen!I', 'Worm VB.AT',\n", "                'Trojan Downloader Wintrim.BX', 'Worm Yuner.A']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["global filename\n", "global knn_precision, nb_precision, tree_precision, svm_precision, random_precision, cnn_precision, lstm_precision\n", "global knn_recall, nb_recall, tree_recall, svm_recall, random_recall, cnn_recall, lstm_recall\n", "global knn_fmeasure, nb_fmeasure, tree_fmeasure, svm_fmeasure, random_fmeasure, cnn_fmeasure, lstm_fmeasure\n", "global knn_acc, nb_acc, tree_acc, svm_acc, random_acc, cnn_acc, lstm_acc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["global classifier\n", "global X_train, X_test, y_train, y_test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_lstmcnn(dataset, standardize=True):\n", "    features = dataset['arr'][:, 0]\n", "    features = np.array([feature for feature in features])\n", "    features = np.reshape(features, (features.shape[0], features.shape[1] * features.shape[2]))\n", "    if standardize:\n", "        features = StandardScaler().fit_transform(features)\n", "    labels = dataset['arr'][:, 1]\n", "    labels = np.array([label for label in labels])\n", "    print(labels.shape)\n", "    print(features.shape)\n", "    return features, labels"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(dataset, standardize=True):\n", "    features = dataset['arr'][:, 0]\n", "    features = np.array([feature for feature in features])\n", "    features = np.reshape(features, (features.shape[0], features.shape[1] * features.shape[2]))\n", "    if standardize:\n", "        features = StandardScaler().fit_transform(features)\n", "    labels = dataset['arr'][:, 1]\n", "    labels = np.array([label for label in labels])\n", "    feature = []\n", "    label = []\n", "    for i in range(0, 4000):\n", "        feature.append(features[i])\n", "        label.append(labels[i])\n", "    feature = np.asarray(feature)\n", "    label = np.asarray(label)\n", "    print(labels.shape)\n", "    print(features.shape)\n", "    print(label.shape)\n", "    print(feature.shape)\n", "    return feature, label"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def upload():\n", "    global filename\n", "    filename = filedialog.askopenfilename(initialdir=\"dataset\")\n", "    pathlabel.config(text=filename)\n", "    text.delete('1.0', END)\n", "    text.insert(END, 'MalImg dataset loaded\\n')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prediction(X_test, cls):\n", "    y_pred = cls.predict(X_test)\n", "    for i in range(len(X_test)):\n", "        print(\"X=%s, Predicted=%s\" % (X_test[i], y_pred[i]))\n", "    return y_pred"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def KNN():\n", "    global knn_precision\n", "    global knn_recall\n", "    global knn_fmeasure\n", "    global knn_acc\n", "    # text.delete('1.0', END)\n", "    cls = KNeighborsClassifier(n_neighbors=10)\n", "    cls.fit(X_train, y_train)\n", "    text.insert(END, \"\\nKNN Prediction Results\\n\")\n", "    prediction_data = prediction(X_test, cls)\n", "    knn_precision = precision_score(y_test, prediction_data, average='micro') * 100\n", "    knn_recall = recall_score(y_test, prediction_data, average='micro') * 100\n", "    knn_fmeasure = f1_score(y_test, prediction_data, average='micro') * 100\n", "    knn_acc = accuracy_score(y_test, prediction_data) * 100\n", "    text.insert(END, \"KNN Precision : \" + str(knn_precision) + \"\\n\")\n", "    text.insert(END, \"KNN Recall : \" + str(knn_recall) + \"\\n\")\n", "    text.insert(END, \"KNN FMeasure : \" + str(knn_fmeasure) + \"\\n\")\n", "    text.insert(END, \"KNN Accuracy : \" + str(knn_acc) + \"\\n\")\n", "    # classifier = cls"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def naivebayes():\n", "    global nb_precision\n", "    global nb_recall\n", "    global nb_fmeasure\n", "    global nb_acc\n", "    # text.delete('1.0', END)\n", "    data, labels = load_data(np.load(filename, allow_pickle=True))\n", "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n", "    scaler = Normalizer().fit(X_train)\n", "    X_train = scaler.transform(X_train)\n", "    scaler = Normalizer().fit(X_test)\n", "    X_test = scaler.transform(X_test)\n", "    cls = BernoulliNB(binarize=0.0)\n", "    cls.fit(X_train, y_train)\n", "    text.insert(END, \"\\nNaive Bayes Prediction Results\\n\\n\")\n", "    prediction_data = prediction(X_test, cls)\n", "    nb_precision = precision_score(y_test, prediction_data, average='micro') * 100\n", "    nb_recall = recall_score(y_test, prediction_data, average='micro') * 100\n", "    nb_fmeasure = f1_score(y_test, prediction_data, average='micro') * 100\n", "    nb_acc = accuracy_score(y_test, prediction_data) * 100\n", "    text.insert(END, \"Naive Bayes Precision : \" + str(nb_precision) + \"\\n\")\n", "    text.insert(END, \"Naive Bayes Recall : \" + str(nb_recall) + \"\\n\")\n", "    text.insert(END, \"Naive Bayes FMeasure : \" + str(nb_fmeasure) + \"\\n\")\n", "    text.insert(END, \"Naive Bayes Accuracy : \" + str(nb_acc) + \"\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def decisionTree():\n", "    # text.delete('1.0', END)\n", "    global tree_acc\n", "    global tree_precision\n", "    global tree_recall\n", "    global tree_fmeasure\n", "    rfc = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"random\", max_depth=20, min_samples_split=50,\n", "                                 min_samples_leaf=20, max_features=5)\n", "    rfc.fit(X_train, y_train)\n", "    text.insert(END, \"\\nDecision Tree Prediction Results\\n\")\n", "    prediction_data = prediction(X_test, rfc)\n", "    tree_precision = precision_score(y_test, prediction_data, average='micro') * 100\n", "    tree_recall = recall_score(y_test, prediction_data, average='micro') * 100\n", "    tree_fmeasure = f1_score(y_test, prediction_data, average='micro') * 100\n", "    tree_acc = accuracy_score(y_test, prediction_data) * 100\n", "    text.insert(END, \"Decision Tree Precision : \" + str(tree_precision) + \"\\n\")\n", "    text.insert(END, \"Decision Tree Recall : \" + str(tree_recall) + \"\\n\")\n", "    text.insert(END, \"Decision Tree FMeasure : \" + str(tree_fmeasure) + \"\\n\")\n", "    text.insert(END, \"Decision Tree Accuracy : \" + str(tree_acc) + \"\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def randomForest():\n", "    # text.delete('1.0', END)\n", "    global random_acc\n", "    global random_precision\n", "    global random_recall\n", "    global random_fmeasure\n", "    rfc = RandomForestClassifier(n_estimators=200, random_state=0)\n", "    rfc.fit(X_train, y_train)\n", "    text.insert(END, \"\\nRandom Forest Prediction Results\\n\")\n", "    prediction_data = prediction(X_test, rfc)\n", "    random_precision = precision_score(y_test, prediction_data, average='micro') * 100\n", "    random_recall = recall_score(y_test, prediction_data, average='micro') * 100\n", "    random_fmeasure = f1_score(y_test, prediction_data, average='micro') * 100\n", "    random_acc = accuracy_score(y_test, prediction_data) * 100\n", "    text.insert(END, \"Random Forest Precision : \" + str(random_precision) + \"\\n\")\n", "    text.insert(END, \"Random Forest Recall : \" + str(random_recall) + \"\\n\")\n", "    text.insert(END, \"Random Forest FMeasure : \" + str(random_fmeasure) + \"\\n\")\n", "    text.insert(END, \"Random Forest Accuracy : \" + str(random_acc) + \"\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def SVM():\n", "    # text.delete('1.0', END)\n", "    global svm_acc\n", "    global svm_precision\n", "    global svm_recall\n", "    global svm_fmeasure\n", "    global X_train, X_test, y_train, y_test\n", "    data, labels = load_data(np.load(filename, allow_pickle=True))\n", "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n", "    print(\"hello\")\n", "    rfc = svm.SVC(C=2.0, gamma='scale', kernel='rbf', random_state=2)\n", "    rfc.fit(X_train, y_train)\n", "    text.insert(END, \"\\nSVM Prediction Results\\n\")\n", "    prediction_data = prediction(X_test, rfc)\n", "    svm_precision = precision_score(y_test, prediction_data, average='micro') * 100\n", "    svm_recall = recall_score(y_test, prediction_data, average='micro') * 100\n", "    svm_fmeasure = f1_score(y_test, prediction_data, average='micro') * 100\n", "    svm_acc = accuracy_score(y_test, prediction_data) * 100\n", "    text.insert(END, \"SVM Precision : \" + str(svm_precision) + \"\\n\")\n", "    text.insert(END, \"SVM Recall : \" + str(svm_recall) + \"\\n\")\n", "    text.insert(END, \"SVM FMeasure : \" + str(svm_fmeasure) + \"\\n\")\n", "    text.insert(END, \"SVM Accuracy : \" + str(svm_acc) + \"\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def LSTM():\n", "    global lstm_acc\n", "    global lstm_precision\n", "    global lstm_recall\n", "    global lstm_fmeasure\n", "    # text.delete('1.0', END)\n", "    data, labels = load_lstmcnn(np.load(filename, allow_pickle=True))\n", "    labels = labels.reshape((9339, 1))\n", "    print(labels)\n", "    data = data.reshape((9339, 32, 32))\n", "    X_train1, X_test1, y_train1, y_test1 = train_test_split(data, labels, test_size=0.101)\n", "    enc = OneHotEncoder()\n", "    enc.fit(y_train1)\n", "    print(y_train1.shape)\n", "    y_train1 = enc.transform(y_train1)\n", "    y_test1 = enc.transform(y_test1)\n", "    # rehsaping traing\n", "    print(\"X_train.shape before  = \", X_train1.shape)\n", "    X_train1 = X_train1.reshape((8395, 32, 32))\n", "    print(\"X_train.shape after  = \", X_train1.shape)\n", "    print(\"y_train.shape  = \", y_train1.shape)\n", "    # rehsaping testing\n", "    print(\"X_test.shape before  = \", X_test1.shape)\n", "    X_test1 = X_test1.reshape((944, 32, 32))\n", "    print(\"X_test.shape after  = \", X_test1.shape)\n", "    print(\"y_test.shape  = \", y_test1.shape)\n", "    model = Sequential()\n", "    model.add(keras.layers.LSTM(100, input_shape=(32, 32)))\n", "    model.add(Dropout(0.5))\n", "    model.add(Dense(100, activation='relu'))\n", "    model.add(Dense(25, activation='softmax'))\n", "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "    print(model.summary())\n", "    model.fit(X_train1, y_train1, epochs=8, batch_size=64)\n", "    prediction_data = model.predict(X_test1)\n", "    prediction_data = np.argmax(prediction_data, axis=1)\n", "    y_test1 = np.argmax(y_test1, axis=1)\n", "    lstm_precision = precision_score(y_test1, prediction_data, average='micro') * 100\n", "    lstm_recall = recall_score(y_test1, prediction_data, average='micro') * 100\n", "    lstm_fmeasure = f1_score(y_test1, prediction_data, average='micro') * 100\n", "    lstm_acc = accuracy_score(y_test1, prediction_data) * 100\n", "    text.insert(END, \"\\nLSTM Prediction Results\\n\")\n", "    text.insert(END, \"LSTM Precision : \" + str(lstm_precision) + \"\\n\")\n", "    text.insert(END, \"LSTM Recall : \" + str(lstm_recall) + \"\\n\")\n", "    text.insert(END, \"LSTM FMeasure : \" + str(lstm_fmeasure) + \"\\n\")\n", "    text.insert(END, \"LSTM Accuracy : \" + str(lstm_acc) + \"\\n\")\n", "    # scores = model.evaluate(X_test, y_test, verbose=0)\n", "    # print(\"Accuracy: %.2f%%\" % (scores[1]*100))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def CNN():\n", "    global cnn_acc\n", "    global cnn_precision\n", "    global cnn_recall\n", "    global cnn_fmeasure\n", "    # text.delete('1.0', END)\n", "    data, labels = load_lstmcnn(np.load(filename, allow_pickle=True))\n", "    labels = labels.reshape((9339, 1))\n", "    print(labels)\n", "    data = data.reshape((9339, 32, 32))\n", "    X_train1, X_test1, y_train1, y_test1 = train_test_split(data, labels, test_size=0.101)\n", "    enc = OneHotEncoder()\n", "    enc.fit(y_train1)\n", "    print(y_train1.shape)\n", "    y_train1 = enc.transform(y_train1)\n", "    y_test1 = enc.transform(y_test1)\n", "    # rehsaping traing\n", "    print(\"X_train.shape before  = \", X_train1.shape)\n", "    X_train1 = X_train1.reshape((8395, 32, 32, 1))\n", "    print(\"X_train.shape after  = \", X_train1.shape)\n", "    print(\"y_train.shape  = \", y_train1.shape)\n", "    # rehsaping testing\n", "    print(\"X_test.shape before  = \", X_test1.shape)\n", "    X_test1 = X_test1.reshape((944, 32, 32, 1))\n", "    print(\"X_test.shape after  = \", X_test1.shape)\n", "    print(\"y_test.shape  = \", y_test1.shape)\n", "    classifier = Sequential()\n", "    classifier.add(Convolution2D(32, (3, 3), border_mode='valid', input_shape=(32, 32, 1)))\n", "    classifier.add(BatchNormalization())\n", "    classifier.add(Activation(\"relu\"))\n", "    classifier.add(Convolution2D(32, (3, 3), border_mode='valid'))\n", "    classifier.add(BatchNormalization())\n", "    classifier.add(Activation(\"relu\"))\n", "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n", "    classifier.add(Flatten())\n", "    classifier.add(Dense(128))\n", "    classifier.add(BatchNormalization())\n", "    classifier.add(Activation(\"relu\"))\n", "    classifier.add(Dense(25))\n", "    classifier.add(BatchNormalization())\n", "    classifier.add(Activation(\"softmax\"))\n", "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n", "    classifier.fit(X_train1, y_train1, epochs=10, batch_size=64)\n", "    prediction_data = classifier.predict(X_test1)\n", "    prediction_data = np.argmax(prediction_data, axis=1)\n", "    y_test1 = np.argmax(y_test1, axis=1)\n", "    cnn_precision = precision_score(y_test1, prediction_data, average='micro') * 100\n", "    cnn_recall = recall_score(y_test1, prediction_data, average='micro') * 100\n", "    cnn_fmeasure = f1_score(y_test1, prediction_data, average='micro') * 100\n", "    cnn_acc = accuracy_score(y_test1, prediction_data) * 100\n", "    text.insert(END, \"\\n CNN Prediction Results\\n\")\n", "    text.insert(END, \"CNN Precision : \" + str(cnn_precision) + \"\\n\")\n", "    text.insert(END, \"CNN Recall : \" + str(cnn_recall) + \"\\n\")\n", "    text.insert(END, \"CNN FMeasure : \" + str(cnn_fmeasure) + \"\\n\")\n", "    text.insert(END, \"CNN Accuracy : \" + str(cnn_acc) + \"\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict():\n", "    filename = filedialog.askopenfilename(initialdir=\"images\")\n", "    # text.delete('1.0', END)\n", "    text.insert(END, filename + \" loaded\\n\\n\")\n", "    with open('model.json', \"r\") as json_file:\n", "        loaded_model_json = json_file.read()\n", "        loaded_model = model_from_json(loaded_model_json)\n", "    loaded_model.load_weights(\"model_weights.h5\")\n", "    loaded_model._make_predict_function()\n", "    print(loaded_model.summary())\n", "    img = np.load(filename)\n", "    im2arr = img.reshape(1, 32, 32, 1)\n", "    preds = loaded_model.predict(im2arr)\n", "    print(str(preds) + \" \" + str(np.argmax(preds)))\n", "    predict = np.argmax(preds)\n", "    text.insert(END, 'Uploaded file contains malware from family : ' + malware_name[predict])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def precisionGraph():\n", "    height = [knn_precision, nb_precision, tree_precision, svm_precision, random_precision,\n", "              cnn_precision, lstm_precision]\n", "    bars = (\n", "    'KNN Precision', 'NB Precision', 'DT Precision', 'SVM Precision', 'RF Precision', 'CNN Precision',\n", "    'LSTM Precision')\n", "    y_pos = np.arange(len(bars))\n", "    plt.bar(y_pos, height)\n", "    plt.xticks(y_pos, bars)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def recallGraph():\n", "    height = [knn_recall, nb_recall, tree_recall, svm_recall, random_recall, cnn_recall, lstm_recall]\n", "    bars = ('KNN Recall', 'NB Recall', 'DT Recall', 'SVM Recall', 'RF Recall', 'CNN Recall', 'LSTM Recall')\n", "    y_pos = np.arange(len(bars))\n", "    plt.bar(y_pos, height)\n", "    plt.xticks(y_pos, bars)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def fscoreGraph():\n", "    height = [knn_fmeasure, nb_fmeasure, tree_fmeasure, svm_fmeasure, random_fmeasure, cnn_fmeasure,\n", "              lstm_fmeasure]\n", "    bars = ('KNN FScore', 'NB FScore', 'DT FScore', 'SVM FScore', 'RF FScore', 'CNN FScore', 'LSTM FScore')\n", "    y_pos = np.arange(len(bars))\n", "    plt.bar(y_pos, height)\n", "    plt.xticks(y_pos, bars)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def accuracyGraph():\n", "    height = [knn_acc, nb_acc, tree_acc, svm_acc, random_acc, cnn_acc, lstm_acc]\n", "    bars = ('KNN ACC', 'NB ACC', 'DT ACC', 'SVM ACC', 'RF ACC', 'CNN ACC', 'LSTM ACC')\n", "    y_pos = np.arange(len(bars))\n", "    plt.bar(y_pos, height)\n", "    plt.xticks(y_pos, bars)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["font = ('times', 16, 'bold')\n", "title = Label(main, text='Robust Intelligent Malware Detection Using Deep Learning')\n", "title.config(bg='gray56', fg='white')\n", "title.config(font=font)\n", "title.config(height=3, width=120)\n", "title.place(x=0, y=5)\n", "#\n", "font1 = ('times', 14, 'bold')\n", "upload = Button(main, text=\"Upload Malware Dataset\", command=upload)\n", "upload.place(x=700, y=100)\n", "upload.config(font=font1)\n", "#\n", "pathlabel = Label(main)\n", "pathlabel.config(bg='gray56', fg='black')\n", "pathlabel.config(font=font1)\n", "pathlabel.place(x=700, y=150)\n", "#\n", "def ML() :\n", "    SVM()\n", "    KNN()\n", "    naivebayes()\n", "    decisionTree()\n", "    randomForest()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def DL() :\n", "    CNN()\n", "    LSTM()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["svmButton = Button(main, text=\"RUN ML ALGORITHMS\", command = ML)\n", "svmButton.place(x=700, y=200)\n", "svmButton.config(font=font1)\n", "#\n", "knnButton = Button(main, text=\"RUN DL Algorithm\", command=DL)\n", "knnButton.place(x=700, y=250)\n", "knnButton.config(font=font1)\n", "#\n", "# nbButton = Button(main, text=\"Naive Bayes Algorithm\", command=naivebayes)\n", "# nbButton.place(x=700, y=300)\n", "# nbButton.config(font=font1)\n", "#\n", "# treeButton = Button(main, text=\"Decision Tree Algorithm\", command=decisionTree)\n", "# treeButton.place(x=700, y=350)\n", "# treeButton.config(font=font1)\n", "#\n", "# randomButton = Button(main, text=\"Random Forest Algorithm\", command=randomForest)\n", "# randomButton.place(x=700, y=450)\n", "# randomButton.config(font=font1)\n", "#\n", "# cnnButton = Button(main, text=\"CNN\", command=CNN)\n", "# cnnButton.place(x=700, y=500)\n", "# cnnButton.config(font=font1)\n", "#\n", "# lstmButton = Button(main, text=\"LSTM\", command=LSTM)\n", "# lstmButton.place(x=900, y=500)\n", "# lstmButton.config(font=font1)\n", "#\n", "graphButton = Button(main, text=\"Precision Graph\", command=precisionGraph)\n", "graphButton.place(x=700, y=300)\n", "graphButton.config(font=font1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["recallButton = Button(main, text=\"Recall Graph\", command=recallGraph)\n", "recallButton.place(x=700, y=350)\n", "recallButton.config(font=font1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scoreButton = Button(main, text=\"Fscore Graph\", command=fscoreGraph)\n", "scoreButton.place(x=700, y=400)\n", "scoreButton.config(font=font1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accButton = Button(main, text=\"Accuracy Graph\", command=accuracyGraph)\n", "accButton.place(x=700, y=450)\n", "accButton.config(font=font1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predictButton = Button(main, text=\"Predict Malware Family\", command=predict)\n", "predictButton.place(x=700, y=500)\n", "predictButton.config(font=font1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["font1 = ('times', 12, 'bold')\n", "text = Text(main, height=30, width=80)\n", "scroll = Scrollbar(text)\n", "text.configure(yscrollcommand=scroll.set)\n", "text.place(x=10, y=100)\n", "text.config(font=font1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["main.config(bg='gray89')\n", "main.mainloop()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}